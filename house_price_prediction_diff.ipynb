{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Jan 25 17:11:08 2019\n",
    "\n",
    "@author: mgelman\n",
    "\"\"\"\n",
    "#fafe\n",
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import graphviz\n",
    "from scipy.sparse import hstack\n",
    "from sklearn import tree\n",
    "import pydot\n",
    "import pydotplus\n",
    "from sklearn.externals.six import StringIO\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.path.expanduser(\"~\"), \"Documents\", \"lowe_house_price_prediction\", \"yelp_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "path = os.path.join(os.path.expanduser(\"~\"), \"Documents\", \"lowe_house_price_prediction\", \"yelp_dataset\")\n",
    "#sys.path.insert(0, path)\n",
    "#sys.path.insert(0, \"C:\\Users\\mgelman\\AppData\\Local\\Continuum\\anaconda2\\Library\\bin\\graphviz\")\n",
    "\n",
    "# STEP 1: LOADING IN DATA\n",
    "\n",
    "# LOAD BUSINESS DATA\n",
    "inputfile = os.path.join(path, \"business.json\")\n",
    "business_data = pd.read_json(inputfile, lines=True)\n",
    "# keep a subset of variables\n",
    "business_data = business_data[['business_id', 'postal_code', 'state', 'categories']]\n",
    "# trim the category text\n",
    "business_data['categories'] = business_data['categories'].str.slice(stop=50)\n",
    "business_data['categories'] = business_data['categories'].str.encode('ascii', errors='coerce')\n",
    "#business_data['categories'] = business_data['categories'].str.replace('&', '')\n",
    "# convert postal code to integers (will get rid of non-us zipcodes)\n",
    "business_data['postal_code'] = pd.to_numeric(business_data['postal_code'], errors='coerce')\n",
    "business_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD IN REVIEW DATA (use unix command top to create a smaller version of the dataset)\n",
    "# head -n NUMBEROFLINES file.json > mynewfile.json\n",
    "# TO GET SPECIFIC YEAR\n",
    "# grep -E '201[0|1|2]' review.json > review_2010-2012.json\n",
    "# inputfile=os.path.join(path,\"review.json\")\n",
    "inputfile = os.path.join(path, \"review_10k.json\")\n",
    "outputfile = os.path.join(path, \"review.pkl\")\n",
    "review_data = pd.read_json(inputfile, lines=True)\n",
    "# only keep business_id, date, stars, text\n",
    "review_data = review_data[['business_id', 'date', 'stars', 'text']]\n",
    "# take out #s\n",
    "review_data['text'] = review_data['text'].str.replace('[0-9]', '')\n",
    "\n",
    "# vectorizer = TfidfVectorizer(ngram_range=(1, 1),\n",
    "#                             stop_words='english',\n",
    "#                             min_df=30)\n",
    "# X_test= vectorizer.fit_transform(review_data['text'])\n",
    "# names=vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputfile = os.path.join(path, \"Zip_Zhvi_AllHomes.csv\")\n",
    "# zillow_data = pd.read_csv(inputfile, header=0, quoting=3)\n",
    "zillow_data = pd.read_csv(inputfile, encoding='latin-1')\n",
    "#zillow_data = zillow_data.head(100)\n",
    "# Drop some columns\n",
    "zillow_data.drop(['RegionID', 'City', 'State', 'Metro', 'CountyName', 'SizeRank'], axis=1, inplace=True)\n",
    "# convert to long\n",
    "zillow_data = pd.melt(zillow_data, id_vars='RegionName', value_vars=list(zillow_data.columns.values)[1:])\n",
    "# generate datetime\n",
    "zillow_data['date'] = pd.to_datetime(zillow_data['variable'])\n",
    "zillow_data['year'] = zillow_data['date'].dt.year\n",
    "# get yearly mean\n",
    "zillow_yearly_avg = zillow_data.groupby(['RegionName', 'year'])['value'].mean().reset_index()\n",
    "\n",
    "# %% Merge everyting together\n",
    "\n",
    "# First merge the yelp data to get zipcode\n",
    "yelp_merged = review_data.merge(business_data, left_on='business_id', right_on='business_id', how='left',\n",
    "                                validate=\"m:1\")\n",
    "yelp_merged['year'] = pd.to_datetime(yelp_merged['date']).dt.year\n",
    "\n",
    "# merge yelp with zillow\n",
    "final_data = yelp_merged.merge(zillow_yearly_avg, left_on=['postal_code', 'year'], right_on=['RegionName', 'year'],\n",
    "                               how='left', validate=\"m:1\")\n",
    "final_data.dropna(inplace=True)\n",
    "\n",
    "csv_file_name = os.path.join(path, \"yelp_zillow.tsv\")\n",
    "final_data[['year', 'stars', 'postal_code', 'state', 'value']].to_csv(csv_file_name, sep='\\t', encoding='utf-8',\n",
    "                                                                      index=False, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% create differences\n",
    "\n",
    "\n",
    "# First aggregate to zip year\n",
    "# create the dictionary for the collapse (mapping from var to agg type)\n",
    "\n",
    "# group by creates a mapping from the user_id to the row index. Ie, user 1 maps to rows 4,5,6, user 2 maps to rows 7,8,9.\n",
    "data_group = final_data.groupby(['postal_code', 'year'], as_index=False)\n",
    "\n",
    "# aggregate in groups depending on the var type\n",
    "# group 1: numerical variables\n",
    "mean = ['value', 'stars']\n",
    "\n",
    "# group 2: retain all strings\n",
    "all = ['text']\n",
    "\n",
    "# group 3: ID vars: like state\n",
    "first = ['state']\n",
    "\n",
    "# create the dictionary for the collapse (mapping from var to agg type)\n",
    "d = {}\n",
    "d_agg_type = {'first': first,\n",
    "              np.nanmean: mean,\n",
    "              lambda x: \" \".join(x): all}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for key, value in d_agg_type.items():\n",
    "    for col in value:\n",
    "        d.update({col: key})\n",
    "\n",
    "# collapse data\n",
    "final_data_collapsed = data_group.agg(d)\n",
    "\n",
    "# create log prices\n",
    "final_data_collapsed['log_price'] = np.log(final_data_collapsed['value'])\n",
    "\n",
    "# NEED TO SORT TO GET DIFF\n",
    "final_data_collapsed.sort_values(['postal_code', 'year'], inplace=True)\n",
    "\n",
    "# convert words to word differences\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1),\n",
    "                             stop_words='english',\n",
    "                             min_df=30)\n",
    "X_test = vectorizer.fit_transform(final_data_collapsed['text'])\n",
    "\n",
    "# add the zipcode and year to the words\n",
    "word_df = pd.concat([final_data_collapsed[['postal_code', 'year']], pd.DataFrame(X_test.todense())], axis=1)\n",
    "# take the difference\n",
    "word_df = word_df.groupby(['postal_code']).diff()\n",
    "# only keep the ones\n",
    "word_df = word_df.loc[word_df['year'] == 1]\n",
    "# drop the column and covert back to sparse matrix\n",
    "word_df.drop(['year'], axis=1, inplace=True)\n",
    "X_words = scipy.sparse.csr_matrix(word_df.values)\n",
    "\n",
    "# sort by postal_code,year\n",
    "final_data_collapsed[['D_LP', 'D_stars', 'D_year']] = final_data_collapsed.groupby(['postal_code'])[\n",
    "    'log_price', 'stars', 'year'].diff()\n",
    "# only keep if the difference in year is 1. Or else we are missing data\n",
    "final_data_collapsed = final_data_collapsed.loc[final_data_collapsed['D_year'] == 1]  # type: object\n",
    "\n",
    "# Create P_LD categories\n",
    "final_data_collapsed['P_LD_CAT'] = pd.qcut(final_data_collapsed['D_LP'].values, 5).codes\n",
    "# confirm the correct split\n",
    "final_data_collapsed['P_LD_CAT'].value_counts(normalize=True, sort=False)\n",
    "# log price describe\n",
    "final_data_collapsed['D_LP'].describe()\n",
    "# state\n",
    "final_data_collapsed['state'].value_counts(normalize=True, sort=True)\n",
    "# histogram\n",
    "final_data_collapsed.hist(column='D_LP', bins=100)\n",
    "plt.show()\n",
    "del review_data, business_data, zillow_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(sorted_x[x])? (<ipython-input-24-73bd95bf7183>, line 77)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-73bd95bf7183>\"\u001b[0;36m, line \u001b[0;32m77\u001b[0m\n\u001b[0;31m    print sorted_x[x]\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(sorted_x[x])?\n"
     ]
    }
   ],
   "source": [
    "# use one hot encoding\n",
    "# final_data = pd.concat([final_data,pd.get_dummies(final_data['categories'])],axis=1)\n",
    "\n",
    "# test=pd.get_dummies(final_data['categories'])\n",
    "\n",
    "# %% Create testing,training dataset\n",
    "\n",
    "# data.to_pickle(outputfile)\n",
    "\n",
    "# define y as the stars and X as text\n",
    "# y_data=final_data['value']\n",
    "y_data = final_data_collapsed['P_LD_CAT']\n",
    "# X_data=final_data[['stars','text']]\n",
    "# X_data=final_data[['stars','text','categories']]\n",
    "X_data = final_data_collapsed[['D_stars']]\n",
    "# print y_data.value_counts(normalize=True, sort=False)\n",
    "\n",
    "# split into training and test\n",
    "X_train, X_test, y_train, y_test, X_words_train, X_words_test = train_test_split(\n",
    "    X_data, y_data, X_words, test_size=0.25, random_state=1234)\n",
    "\n",
    "vocab = vectorizer.get_feature_names()\n",
    "vocab_str = [str(x.encode('utf-8')) for x in vocab]\n",
    "\n",
    "# Add in stars and yelp categories as features\n",
    "X_tr = hstack([X_words_train, np.matrix(X_train)])\n",
    "X_te = hstack([X_words_test, np.matrix(X_test)])\n",
    "\n",
    "# %%\n",
    "\n",
    "# falsification test\n",
    "# y_train=y_train.sample(frac=1)\n",
    "\n",
    "# Use the tree clasifier\n",
    "# clf = DecisionTreeRegressor(max_leaf_nodes=15)\n",
    "clf = DecisionTreeClassifier()\n",
    "# clf = DecisionTreeClassifier()\n",
    "# clf = DecisionTreeRegressor()\n",
    "# clf = RandomForestClassifier(\n",
    "#                n_estimators=128,\n",
    "#                n_jobs=-1,\n",
    "#                verbose=1)\n",
    "\n",
    "# clf = DecisionTreeClassifier(max_leaf_nodes=15)\n",
    "\n",
    "clf = clf.fit(X_tr, y_train)\n",
    "\n",
    "# check which words are the most frequent\n",
    "# sum_words = X_tr.sum(axis=0)\n",
    "# words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "# words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "# \n",
    "##Look a Top X frequent words\n",
    "# for word, freq in words_freq[:10]:\n",
    "#    print(word, freq)  \n",
    "\n",
    "\n",
    "y_pred = clf.predict(X_te)\n",
    "\n",
    "# confusion matrix\n",
    "# create label\n",
    "price_label = [0, 1, 2, 3, 4]\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "plot_confusion_matrix(cm, classes=price_label, normalize=True, title='Normalized confusion matrix')\n",
    "\n",
    "# print cm\n",
    "# print cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "# features are the text labels, then STARS, then the yelp categories\n",
    "# feat_str=vocab_str+[\"stars\"]+list(X_train.columns.values)[2:]\n",
    "feat_str = vocab_str + [\"stars\"]\n",
    "dictionary = dict(zip(feat_str, clf.feature_importances_))\n",
    "sorted_x = sorted(dictionary.items(), key=operator.itemgetter(1), reverse=True)\n",
    "for x in range(20):\n",
    "    print sorted_x[x]\n",
    "\n",
    "# overall score\n",
    "training_score = clf.score(X_tr, y_train, sample_weight=None)\n",
    "testing_score = clf.score(X_te, y_test, sample_weight=None)\n",
    "print()\n",
    "print(\"the training_score is \" + str(training_score))\n",
    "print()\n",
    "print(\"the testing_score is \" + str(testing_score))\n",
    "\n",
    "# %%\n",
    "# Graph the tree\n",
    "# os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "\n",
    "dot_data = StringIO()\n",
    "# graphfile=os.path.join(path,\"fig\",\"graph_v2.dot\")\n",
    "# tree.export_graphviz(clf, out_file=graphfile,\n",
    "tree.export_graphviz(clf, out_file=dot_data,\n",
    "                     feature_names=feat_str,\n",
    "                     filled=True, rounded=True, special_characters=True,\n",
    "                     # proportion=True\n",
    "                     )\n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "graphfile = os.path.join(path, \"fig\", \"graph_v2.pdf\")\n",
    "graph.write_pdf(graphfile)\n",
    "\n",
    "# %%\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "#\n",
    "# clf = MultinomialNB().fit(X_tr, y_train)  # classifying transformed text data to target value\n",
    "#\n",
    "# # confusion matrix\n",
    "# y_pred = clf.predict(X_te)\n",
    "# # cm=confusion_matrix(y_test, y_pred)\n",
    "# # np.set_printoptions(precision=2)\n",
    "# # plot_confusion_matrix(cm, classes=star_label,normalize=True,title='Normalized confusion matrix')\n",
    "#\n",
    "#\n",
    "# # calculating the mean accuracy on the given test data and labels\n",
    "# training_score = clf.score(X_tr, y_train, sample_weight=None)\n",
    "# testing_score = clf.score(X_te, y_test, sample_weight=None)\n",
    "# print()\n",
    "# print(\"the training_score is \" + str(training_score))\n",
    "# print()\n",
    "# print(\"the testing_score is \" + str(testing_score))\n",
    "#\n",
    "# neg_class_prob_sorted = clf.feature_log_prob_[0, :].argsort()\n",
    "# pos_class_prob_sorted = clf.feature_log_prob_[4, :].argsort()\n",
    "#\n",
    "# print(np.take(feat_str, neg_class_prob_sorted[-10:]))\n",
    "# print(np.take(feat_str, pos_class_prob_sorted[-10:]))\n",
    "\n",
    "# %%\n",
    "# from sklearn.svm import LinearSVC\n",
    "#\n",
    "# clf = LinearSVC().fit(X_tr, y_train)  # classifying transformed text data to target value\n",
    "#\n",
    "# # confusion matrix\n",
    "# y_pred = clf.predict(X_te)\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# np.set_printoptions(precision=2)\n",
    "# plot_confusion_matrix(cm, classes=price_label, normalize=True, title='Normalized confusion matrix')\n",
    "#\n",
    "# # calculating the mean accuracy on the given test data and labels\n",
    "# training_score = clf.score(X_tr, y_train, sample_weight=None)\n",
    "# testing_score = clf.score(X_te, y_test, sample_weight=None)\n",
    "# print()\n",
    "# print(\"the training_score is \" + str(training_score))\n",
    "# print()\n",
    "# print(\"the testing_score is \" + str(testing_score))\n",
    "#\n",
    "#\n",
    "# def plot_coefficients(classifier, feature_names, top_features=10):\n",
    "#     lowstar_coef = classifier.coef_[0, :] * -1\n",
    "#     highstar_coef = classifier.coef_[4, :]\n",
    "#     top_positive_coefficients = np.argsort(highstar_coef)[-top_features:]\n",
    "#     top_negative_coefficients = np.argsort(lowstar_coef)[:top_features]\n",
    "#     top_coefficients = np.hstack([top_negative_coefficients, top_positive_coefficients])\n",
    "#     # create plot\n",
    "#     plt.figure(figsize=(15, 5))\n",
    "#     colors = ['red' if c < 0 else 'blue' for c in\n",
    "#               np.hstack([lowstar_coef[top_negative_coefficients], highstar_coef[top_positive_coefficients]])]\n",
    "#     plt.bar(np.arange(2 * top_features),\n",
    "#             np.hstack([lowstar_coef[top_negative_coefficients], highstar_coef[top_positive_coefficients]]),\n",
    "#             color=colors)\n",
    "#     feature_names = np.array(feature_names)\n",
    "#     plt.xticks(np.arange(0, 1 + 2 * top_features), feature_names[top_coefficients], rotation=60)\n",
    "#     plt.show()\n",
    "#\n",
    "#\n",
    "# plot_coefficients(clf, feat_str)\n",
    "# # plot_coefficients(clf, vocab,star_type=4)\n",
    "\n",
    "\n",
    "# %%\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "clf = clf.fit(X_tr, y_train)\n",
    "\n",
    "# calculating the mean accuracy on the given test data and labels\n",
    "training_score = clf.score(X_tr, y_train, sample_weight=None)\n",
    "testing_score = clf.score(X_te, y_test, sample_weight=None)\n",
    "print()\n",
    "print(\"the training_score is \" + str(training_score))\n",
    "print()\n",
    "print(\"the testing_score is \" + str(testing_score))\n",
    "\n",
    "# %% Compare all the different types\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    LinearSVC(),\n",
    "    # NuSVC(probability=True),\n",
    "    # GaussianNB(),\n",
    "    # MultinomialNB(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(n_estimators=8, n_jobs=-1, verbose=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "]\n",
    "\n",
    "# Logging for Visual Comparison\n",
    "log_cols = [\"Classifier\", \"Accuracy\"]\n",
    "log = pd.DataFrame(columns=log_cols)\n",
    "\n",
    "denseclass = ['GaussianNB', 'LinearDiscriminantAnalysis', 'QuadraticDiscriminantAnalysis']\n",
    "\n",
    "for clf in classifiers:\n",
    "\n",
    "    name = clf.__class__.__name__\n",
    "    if name in denseclass:\n",
    "        X_train = X_tr.toarray()\n",
    "        X_test = X_te.toarray()\n",
    "    else:\n",
    "        X_train = X_tr\n",
    "        X_test = X_te\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"=\" * 30)\n",
    "    print(name)\n",
    "\n",
    "    print('****Results****')\n",
    "    acc = clf.score(X_test, y_test, sample_weight=None)\n",
    "    print(\"Accuracy: {:.4%}\".format(acc))\n",
    "\n",
    "    log_entry = pd.DataFrame([[name, acc * 100]], columns=log_cols)\n",
    "    log = log.append(log_entry)\n",
    "\n",
    "print(\"=\" * 30)\n",
    "\n",
    "print log.sort_values(by='Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
